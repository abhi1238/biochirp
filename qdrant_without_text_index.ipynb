{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dedbc4f",
   "metadata": {},
   "source": [
    "# VERSION 1: WITHOUT TEXT INDEX (Pure Vector Search - FASTEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qdrant_flat_adapter_NO_TEXT_INDEX.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "import logging\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as qm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ccf41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "with open('resources/embeddings/biochirp_embeddings.pkl', 'rb') as f:\n",
    "    biochirp_embeddings = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704bb1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# Logging\n",
    "# =========================\n",
    "\n",
    "def setup_logging(level: int = logging.INFO) -> logging.Logger:\n",
    "    logger = logging.getLogger(\"qdrant_upload\")\n",
    "    if not logger.handlers:\n",
    "        handler = logging.StreamHandler(stream=sys.stdout)\n",
    "        handler.setFormatter(logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\"))\n",
    "        logger.addHandler(handler)\n",
    "    logger.setLevel(level)\n",
    "    return logger\n",
    "\n",
    "LOG = setup_logging(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a81717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class QdrantConfig:\n",
    "    \n",
    "#     url: str = \"http://localhost:6333\"\n",
    "#     distance: qm.Distance = qm.Distance.COSINE\n",
    "#     create_payload_indexes: bool = True\n",
    "#     index_text_field: bool = False  # ? DISABLED for maximum speed\n",
    "    \n",
    "#     # HNSW optimization parameters\n",
    "#     hnsw_m: int = 16                # edges per node (16 is balanced)\n",
    "#     hnsw_ef_construct: int = 100    # construction quality\n",
    "#     hnsw_ef: int = 64               # search quality (runtime tunable)\n",
    "\n",
    "# def get_client(cfg: QdrantConfig) -> QdrantClient:\n",
    "#     \"\"\"Prefer gRPC for heavy ingest; fall back to HTTP if 6334 is closed.\"\"\"\n",
    "#     try:\n",
    "#         return QdrantClient(\n",
    "#             host=\"localhost\", port=6333, grpc_port=6334,\n",
    "#             prefer_grpc=True, timeout=300.0\n",
    "#         )\n",
    "#     except Exception:\n",
    "#         LOG.warning(\"gRPC unavailable, falling back to HTTP\")\n",
    "#         return QdrantClient(host=\"localhost\", port=6333, prefer_grpc=False, timeout=300.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d04cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class QdrantConfig:\n",
    "    host: str = \"localhost\"  # <-- Change from \"localhost\" to your Docker service/container name\n",
    "    port: int = 6333\n",
    "    grpc_port: int = 6334\n",
    "    distance: qm.Distance = qm.Distance.COSINE\n",
    "    create_payload_indexes: bool = True\n",
    "    index_text_field: bool = False  # ? DISABLED for maximum speed\n",
    "    \n",
    "    # HNSW optimization parameters\n",
    "    hnsw_m: int = 16                # edges per node (16 is balanced)\n",
    "    hnsw_ef_construct: int = 100    # construction quality\n",
    "    hnsw_ef: int = 64               # search quality (runtime tunable)\n",
    "\n",
    "\n",
    "def get_client(cfg: QdrantConfig) -> QdrantClient:\n",
    "    \"\"\"Prefer gRPC for heavy ingest; fall back to HTTP if 6334 is closed.\"\"\"\n",
    "    try:\n",
    "        return QdrantClient(\n",
    "            host=\"localhost\",\n",
    "            port=cfg.port,\n",
    "            grpc_port=cfg.grpc_port,\n",
    "            prefer_grpc=True,\n",
    "            timeout=300.0\n",
    "        )\n",
    "    except Exception:\n",
    "        LOG.warning(\"gRPC unavailable, falling back to HTTP\")\n",
    "        return QdrantClient(\n",
    "            host=\"localhost\",\n",
    "            port=cfg.port,\n",
    "            prefer_grpc=False,\n",
    "            timeout=300.0\n",
    "        )\n",
    "\n",
    "# def get_client(cfg: QdrantConfig) -> QdrantClient:\n",
    "#     \"\"\"Prefer gRPC for heavy ingest; fall back to HTTP if 6334 is closed.\"\"\"\n",
    "#     try:\n",
    "#         return QdrantClient(\n",
    "#             host=\"localhost\", port=6333, grpc_port=6334,\n",
    "#             prefer_grpc=True, timeout=300.0\n",
    "#         )\n",
    "#     except Exception:\n",
    "#         LOG.warning(\"gRPC unavailable, falling back to HTTP\")\n",
    "#         return QdrantClient(host=\"localhost\", port=6333, prefer_grpc=False, timeout=300.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774f1226",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Config / Client (NO TEXT INDEX)\n",
    "# =========================\n",
    "\n",
    "\n",
    "\n",
    "def model_to_collection(model_name: str) -> str:\n",
    "    return f\"emb_{model_name.replace('/', '_')}\"\n",
    "\n",
    "# =========================\n",
    "# Deterministic Point IDs\n",
    "# =========================\n",
    "\n",
    "QDRANT_ID_NAMESPACE = uuid.UUID(\"3e0d7e58-5c5e-4c5e-9a0b-7a6be3f2b9aa\")\n",
    "\n",
    "def make_point_uuid(model: str, db: str, field: str, i: int) -> str:\n",
    "    name = f\"{model}|{db}|{field}|{i}\"\n",
    "    return str(uuid.uuid5(QDRANT_ID_NAMESPACE, name))\n",
    "\n",
    "# =========================\n",
    "# Collection / Index setup (NO TEXT INDEX)\n",
    "# =========================\n",
    "\n",
    "def ensure_collection_for_model(\n",
    "    client: QdrantClient,\n",
    "    model_name: str,\n",
    "    dim: int,\n",
    "    cfg: QdrantConfig,\n",
    ") -> str:\n",
    "    coll = model_to_collection(model_name)\n",
    "    if not client.collection_exists(coll):\n",
    "        LOG.info(f\"[{coll}] creating collection (dim={dim}, distance={cfg.distance})\")\n",
    "        client.create_collection(\n",
    "            collection_name=coll,\n",
    "            vectors_config=qm.VectorParams(size=dim, distance=cfg.distance),\n",
    "            hnsw_config=qm.HnswConfigDiff(\n",
    "                m=cfg.hnsw_m,\n",
    "                ef_construct=cfg.hnsw_ef_construct,\n",
    "                full_scan_threshold=10000,\n",
    "            ),\n",
    "            optimizers_config=qm.OptimizersConfigDiff(\n",
    "                indexing_threshold=20000,\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        # Only create KEYWORD indexes for filtered fields\n",
    "        if cfg.create_payload_indexes:\n",
    "            for field, schema in [\n",
    "                (\"model\", qm.PayloadSchemaType.KEYWORD),\n",
    "                (\"db\",    qm.PayloadSchemaType.KEYWORD),\n",
    "                (\"field\", qm.PayloadSchemaType.KEYWORD),\n",
    "            ]:\n",
    "                LOG.info(f\"[{coll}] creating payload index on '{field}' ({schema})\")\n",
    "                client.create_payload_index(coll, field_name=field, field_schema=schema)\n",
    "            \n",
    "            # ? NO TEXT INDEX - Skipped for maximum retrieval speed\n",
    "            LOG.info(f\"[{coll}] TEXT index on 'text' field: DISABLED (pure vector search)\")\n",
    "    else:\n",
    "        LOG.info(f\"[{coll}] exists (skip create)\")\n",
    "    return coll\n",
    "\n",
    "# =========================\n",
    "# Dimension inference\n",
    "# =========================\n",
    "\n",
    "def infer_dim_by_model_flat(a: Dict[str, Any]) -> Dict[str, int]:\n",
    "    dims: Dict[str, int] = {}\n",
    "    for model, dbs in a.items():\n",
    "        seen = set()\n",
    "        for db in dbs.values():\n",
    "            for field in db.values():\n",
    "                arr = field.get(\"embeddings\") if isinstance(field, dict) else None\n",
    "                if isinstance(arr, np.ndarray) and arr.ndim == 2:\n",
    "                    seen.add(int(arr.shape[1]))\n",
    "        if not seen:\n",
    "            raise ValueError(f\"No embeddings found for model '{model}' in flat-leaf schema.\")\n",
    "        if len(seen) != 1:\n",
    "            raise ValueError(f\"Model '{model}' has mixed dimensions: {sorted(seen)}\")\n",
    "        dims[model] = seen.pop()\n",
    "    return dims\n",
    "\n",
    "# =========================\n",
    "# Chunking helper\n",
    "# =========================\n",
    "\n",
    "def _iter_row_chunks(n_rows: int, chunk_rows: int):\n",
    "    for s in range(0, n_rows, chunk_rows):\n",
    "        e = min(s + chunk_rows, n_rows)\n",
    "        yield s, e\n",
    "\n",
    "# =========================\n",
    "# Upsert with retry + adaptive split\n",
    "# =========================\n",
    "\n",
    "def _try_upsert(\n",
    "    client: QdrantClient,\n",
    "    collection: str,\n",
    "    ids: List[str],\n",
    "    vecs: List[np.ndarray],\n",
    "    payloads: List[Dict[str, Any]],\n",
    ") -> None:\n",
    "    client.upsert(\n",
    "        collection_name=collection,\n",
    "        points=qm.Batch(ids=ids, vectors=np.vstack(vecs), payloads=payloads),\n",
    "        wait=False,\n",
    "    )\n",
    "\n",
    "def _flush_with_retry(\n",
    "    client: QdrantClient,\n",
    "    collection: str,\n",
    "    ids: List[str],\n",
    "    vecs: List[np.ndarray],\n",
    "    payloads: List[Dict[str, Any]],\n",
    "    global_state: Dict[str, Any] | None,\n",
    "    max_retries: int = 5,\n",
    ") -> None:\n",
    "    n = len(ids)\n",
    "    if n == 0:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        t0 = time.perf_counter()\n",
    "        _try_upsert(client, collection, ids, vecs, payloads)\n",
    "        dt = time.perf_counter() - t0\n",
    "        if global_state is not None:\n",
    "            global_state[\"uploaded\"] += n\n",
    "            uploaded = global_state[\"uploaded\"]\n",
    "            total = global_state.get(\"total\", None)\n",
    "            rate = n / dt if dt > 0 else float(\"inf\")\n",
    "            if total:\n",
    "                pct = 100.0 * uploaded / total if total > 0 else 100.0\n",
    "                remaining = max(total - uploaded, 0)\n",
    "                eta = remaining / rate if rate > 0 else float(\"inf\")\n",
    "                LOG.info(\n",
    "                    f\"[{collection}] batch={n} in {dt:.2f}s | rate={rate:,.1f} pts/s | \"\n",
    "                    f\"uploaded={uploaded:,}/{total:,} ({pct:.2f}%) | ETA={eta:.1f}s\"\n",
    "                )\n",
    "            else:\n",
    "                LOG.info(f\"[{collection}] batch={n} in {dt:.2f}s | rate={rate:,.1f} pts/s | uploaded={uploaded:,}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        LOG.warning(f\"[{collection}] upsert failed for batch size {n}: {e!r}\")\n",
    "\n",
    "    left_ids, right_ids = ids[: n // 2], ids[n // 2 :]\n",
    "    left_vecs, right_vecs = vecs[: n // 2], vecs[n // 2 :]\n",
    "    left_pl, right_pl = payloads[: n // 2], payloads[n // 2 :]\n",
    "\n",
    "    delay = 1.0\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            _try_upsert(client, collection, left_ids, left_vecs, left_pl)\n",
    "            if global_state is not None:\n",
    "                global_state[\"uploaded\"] += len(left_ids)\n",
    "            _try_upsert(client, collection, right_ids, right_vecs, right_pl)\n",
    "            if global_state is not None:\n",
    "                global_state[\"uploaded\"] += len(right_ids)\n",
    "            LOG.info(f\"[{collection}] success after split retry (attempt {attempt}) for total {n}\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            LOG.warning(f\"[{collection}] retry {attempt}/{max_retries} failed: {e!r}; sleeping {delay:.1f}s\")\n",
    "            time.sleep(delay)\n",
    "            delay *= 2\n",
    "\n",
    "    if len(left_ids) > 1:\n",
    "        _flush_with_retry(client, collection, left_ids, left_vecs, left_pl, global_state, max_retries=3)\n",
    "    else:\n",
    "        LOG.error(f\"[{collection}] could not upsert even single left point: {left_ids}\")\n",
    "\n",
    "    if len(right_ids) > 1:\n",
    "        _flush_with_retry(client, collection, right_ids, right_vecs, right_pl, global_state, max_retries=3)\n",
    "    else:\n",
    "        LOG.error(f\"[{collection}] could not upsert even single right point: {right_ids}\")\n",
    "\n",
    "def _flush_batch(\n",
    "    client: QdrantClient,\n",
    "    collection: str,\n",
    "    ids: List[str],\n",
    "    vecs: List[np.ndarray],\n",
    "    payloads: List[Dict[str, Any]],\n",
    "    *,\n",
    "    global_state: Dict[str, Any] | None = None,\n",
    ") -> None:\n",
    "    if not ids:\n",
    "        return\n",
    "    _flush_with_retry(client, collection, ids, vecs, payloads, global_state)\n",
    "    ids.clear(); vecs.clear(); payloads.clear()\n",
    "\n",
    "# =========================\n",
    "# Bulk upload with HNSW optimization\n",
    "# =========================\n",
    "\n",
    "def upload_from_flat(\n",
    "    client: QdrantClient,\n",
    "    a: Dict[str, Any],\n",
    "    dim_by_model: Dict[str, int],\n",
    "    cfg: QdrantConfig,\n",
    "    batch_size: int = 5_000,       # ? Increased batch size\n",
    "    use_tqdm: bool = True,\n",
    "    chunk_rows: int = 100_000,\n",
    "    cooldown_after_timeouts: float = 0.5,\n",
    ") -> None:\n",
    "    try:\n",
    "        from tqdm import tqdm\n",
    "    except Exception:\n",
    "        tqdm = None\n",
    "        use_tqdm = False\n",
    "\n",
    "    for model_name, dbs in a.items():\n",
    "        coll = ensure_collection_for_model(client, model_name, dim_by_model[model_name], cfg)\n",
    "        \n",
    "        # ? DISABLE HNSW DURING UPLOAD (massive speedup)\n",
    "        LOG.info(f\"[{coll}] Disabling HNSW indexing for fast upload...\")\n",
    "        client.update_collection(\n",
    "            collection_name=coll,\n",
    "            hnsw_config=qm.HnswConfigDiff(m=0)\n",
    "        )\n",
    "        \n",
    "        total_points = 0\n",
    "        for db_name, fields in dbs.items():\n",
    "            for field_name, leaf in fields.items():\n",
    "                if isinstance(leaf, dict) and isinstance(leaf.get(\"embeddings\"), np.ndarray):\n",
    "                    arr = leaf[\"embeddings\"]\n",
    "                    if arr.ndim == 2:\n",
    "                        total_points += arr.shape[0]\n",
    "        LOG.info(f\"[{coll}] planned points: {total_points:,}\")\n",
    "\n",
    "        global_state = {\"uploaded\": 0, \"total\": total_points}\n",
    "        pbar = tqdm(total=total_points, desc=f\"{coll}\", unit=\"pt\") if (use_tqdm and tqdm and total_points > 0) else None\n",
    "\n",
    "        t_model_start = time.perf_counter()\n",
    "        for db_name, fields in dbs.items():\n",
    "            for field_name, leaf in fields.items():\n",
    "                if not isinstance(leaf, dict):\n",
    "                    continue\n",
    "                arr = leaf.get(\"embeddings\")\n",
    "                texts = leaf.get(\"texts\", [])\n",
    "                metas = leaf.get(\"metadata\", [])\n",
    "\n",
    "                if not (isinstance(arr, np.ndarray) and arr.ndim == 2):\n",
    "                    continue\n",
    "                if arr.dtype != np.float32:\n",
    "                    arr = arr.astype(np.float32)\n",
    "\n",
    "                n = arr.shape[0]\n",
    "                if not isinstance(texts, list) or len(texts) != n:\n",
    "                    texts = [\"\" for _ in range(n)]\n",
    "                if not isinstance(metas, list) or len(metas) != n:\n",
    "                    metas = [{} for _ in range(n)]\n",
    "\n",
    "                LOG.info(f\"[{coll}] slice model={model_name} db={db_name} field={field_name} rows={n:,}\")\n",
    "\n",
    "                for s, e in _iter_row_chunks(n, chunk_rows):\n",
    "                    LOG.info(f\"[{coll}]  chunk rows {s:,}:{e:,} (size={e-s:,})\")\n",
    "                    ids, vecs, payloads = [], [], []\n",
    "                    timeouts_in_chunk = 0\n",
    "\n",
    "                    for i in range(s, e):\n",
    "                        pid = make_point_uuid(model_name, db_name, field_name, i)\n",
    "                        ids.append(pid)\n",
    "                        vecs.append(arr[i])\n",
    "                        row_meta = metas[i] if isinstance(metas[i], dict) else {}\n",
    "                        payloads.append({\n",
    "                            \"model\": model_name,\n",
    "                            \"db\": db_name,\n",
    "                            \"field\": field_name,\n",
    "                            \"text\": texts[i],\n",
    "                            \"composite_id\": f\"{model_name}|{db_name}|{field_name}|{i}\",\n",
    "                            **row_meta,\n",
    "                        })\n",
    "\n",
    "                        if len(ids) >= batch_size:\n",
    "                            try:\n",
    "                                _flush_batch(client, coll, ids, vecs, payloads, global_state=global_state)\n",
    "                                if pbar: pbar.update(batch_size)\n",
    "                            except Exception as e:\n",
    "                                timeouts_in_chunk += 1\n",
    "                                LOG.warning(f\"[{coll}] batch flush bubbled exception: {e!r}\")\n",
    "                                batch_size = max(256, batch_size // 2)\n",
    "                                time.sleep(cooldown_after_timeouts)\n",
    "\n",
    "                    try:\n",
    "                        _flush_batch(client, coll, ids, vecs, payloads, global_state=global_state)\n",
    "                        if pbar:\n",
    "                            already = pbar.n\n",
    "                            target = min(global_state[\"uploaded\"], total_points)\n",
    "                            if target > already:\n",
    "                                pbar.update(target - already)\n",
    "                    except Exception as e:\n",
    "                        timeouts_in_chunk += 1\n",
    "                        LOG.warning(f\"[{coll}] remainder flush bubbled exception: {e!r}\")\n",
    "                        batch_size = max(256, batch_size // 2)\n",
    "                        time.sleep(cooldown_after_timeouts)\n",
    "\n",
    "                    if timeouts_in_chunk > 0 and batch_size > 256:\n",
    "                        LOG.info(f\"[{coll}] reducing batch_size to {batch_size} due to timeouts in chunk\")\n",
    "\n",
    "        if pbar:\n",
    "            pbar.close()\n",
    "\n",
    "        #  RE-ENABLE HNSW AFTER UPLOAD (builds index once)\n",
    "        LOG.info(f\"[{coll}] Re-enabling HNSW indexing...\")\n",
    "        client.update_collection(\n",
    "            collection_name=coll,\n",
    "            hnsw_config=qm.HnswConfigDiff(\n",
    "                m=cfg.hnsw_m,\n",
    "                ef_construct=cfg.hnsw_ef_construct\n",
    "            )\n",
    "        )\n",
    "\n",
    "        dt_model = time.perf_counter() - t_model_start\n",
    "        up = global_state[\"uploaded\"]\n",
    "        r = up / dt_model if dt_model > 0 else float(\"inf\")\n",
    "        LOG.info(f\"[{coll}] done: uploaded={up:,}/{total_points:,} in {dt_model:.2f}s ({r:,.1f} pts/s)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d1d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = QdrantConfig(url=\"http://localhost:6333\")\n",
    "cfg = QdrantConfig(host=\"localhost\", port=6333, grpc_port=6334)\n",
    "\n",
    "client = get_client(cfg)\n",
    "\n",
    "dim_by_model = infer_dim_by_model_flat(biochirp_embeddings)\n",
    "\n",
    "# ? Upload with HNSW disabled during ingest, re-enabled after\n",
    "upload_from_flat(client, biochirp_embeddings, dim_by_model, cfg, batch_size=1024)\n",
    "\n",
    "# Example search\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_names = list(dim_by_model.keys())\n",
    "model_cache = {name: SentenceTransformer(name) for name in model_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4305be69",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486cdeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Fast Search (No Text Filtering)\n",
    "# =========================\n",
    "\n",
    "def search_fast(\n",
    "    client: QdrantClient,\n",
    "    collection_name: str,\n",
    "    query_vector: List[float],\n",
    "    limit: int = 100,\n",
    "    hnsw_ef: int = 64,  # ? Tune this: 32=fastest, 128=more accurate\n",
    "    filter_conditions: Optional[qm.Filter] = None,\n",
    ") -> List[qm.ScoredPoint]:\n",
    "    \"\"\"\n",
    "    Fast vector similarity search without text filtering.\n",
    "    \"\"\"\n",
    "    return client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        search_params=qm.SearchParams(\n",
    "            hnsw_ef=hnsw_ef,\n",
    "            exact=False,\n",
    "        ),\n",
    "        limit=limit,\n",
    "        query_filter=filter_conditions,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1432a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def search_fast(\n",
    "#     client: QdrantClient,\n",
    "#     collection_name: str,\n",
    "#     query_vector: List[float],\n",
    "#     model_name: str,\n",
    "#     db_name: str,\n",
    "#     field_name: str,\n",
    "#     limit: int = 100,\n",
    "#     hnsw_ef: int = 64,\n",
    "# ) -> List[qm.ScoredPoint]:\n",
    "#     \"\"\"\n",
    "#     Fast vector similarity search without text filtering.\n",
    "#     \"\"\"\n",
    "#     from kneed import KneeLocator\n",
    "#     # ? Correct filter pattern matching your function\n",
    "#     flt = qm.Filter(must=[\n",
    "#         qm.FieldCondition(key=\"model\", match=qm.MatchValue(value=model_name)),\n",
    "#         qm.FieldCondition(key=\"db\", match=qm.MatchValue(value=db_name)),\n",
    "#         qm.FieldCondition(key=\"field\", match=qm.MatchValue(value=field_name)),\n",
    "#     ])\n",
    "    \n",
    "#     return client.search(\n",
    "#         collection_name=collection_name,\n",
    "#         query_vector=query_vector,\n",
    "#         search_params=qm.SearchParams(\n",
    "#             hnsw_ef=hnsw_ef,  # ? 32=fastest, 64=balanced, 128=accurate\n",
    "#             exact=False,\n",
    "#         ),\n",
    "#         limit=limit,\n",
    "#         with_payload=True,\n",
    "#         query_filter=flt,\n",
    "#     )\n",
    "\n",
    "\n",
    "# def search_reference_term_all_models_FAST(\n",
    "#     client: QdrantClient,\n",
    "#     reference_term: str,\n",
    "#     target_field: str,\n",
    "#     model_cache: Dict[str, \"SentenceTransformer\"],\n",
    "#     limit_per_model: int = 50,\n",
    "#     use_knee_cutoff: bool = True,\n",
    "#     db_whitelist: Optional[List[str]] = None,\n",
    "#     hnsw_ef: int = 64,  # ? NEW: Tunable search speed parameter\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Fast version with optimized HNSW search parameters.\n",
    "#     \"\"\"\n",
    "#     from kneed import KneeLocator\n",
    "#     rows: List[Dict[str, Any]] = []\n",
    "\n",
    "#     for model_name, model in model_cache.items():\n",
    "#         coll = model_to_collection(model_name)\n",
    "#         if not client.collection_exists(coll):\n",
    "#             LOG.warning(f\"[{coll}] collection missing; skipping model='{model_name}'\")\n",
    "#             continue\n",
    "\n",
    "#         q_vec = model.encode(\n",
    "#             reference_term,\n",
    "#             convert_to_tensor=True,\n",
    "#             normalize_embeddings=True\n",
    "#         )\n",
    "\n",
    "#         dbs = db_whitelist if db_whitelist is not None else distinct_values(client, coll, \"db\")\n",
    "\n",
    "#         for db_name in dbs:\n",
    "#             # ? Correct filter pattern\n",
    "#             flt = qm.Filter(must=[\n",
    "#                 qm.FieldCondition(key=\"model\", match=qm.MatchValue(value=model_name)),\n",
    "#                 qm.FieldCondition(key=\"db\", match=qm.MatchValue(value=db_name)),\n",
    "#                 qm.FieldCondition(key=\"field\", match=qm.MatchValue(value=target_field)),\n",
    "#             ])\n",
    "\n",
    "#             hits = client.search(\n",
    "#                 collection_name=coll,\n",
    "#                 query_vector=q_vec,\n",
    "#                 limit=limit_per_model,\n",
    "#                 with_payload=True,\n",
    "#                 query_filter=flt,\n",
    "#                 search_params=qm.SearchParams(\n",
    "#                     hnsw_ef=hnsw_ef,  # ? Speed optimization\n",
    "#                     exact=False,\n",
    "#                 ),\n",
    "#             )\n",
    "            \n",
    "#             if not hits:\n",
    "#                 continue\n",
    "\n",
    "#             scores = np.array([h.score for h in hits], dtype=np.float32)\n",
    "#             if use_knee_cutoff and len(scores) > 1:\n",
    "#                 sorted_scores = np.sort(scores)[::-1]\n",
    "#                 try:\n",
    "#                     knee = KneeLocator(range(len(sorted_scores)), sorted_scores,\n",
    "#                                        curve=\"convex\", direction=\"decreasing\", S=0.5)\n",
    "#                     threshold = float(sorted_scores[knee.knee])\n",
    "#                 except Exception:\n",
    "#                     threshold = 0.0\n",
    "#             else:\n",
    "#                 threshold = 0.0\n",
    "\n",
    "#             for h in hits:\n",
    "#                 if h.score >= threshold:\n",
    "#                     pl = h.payload or {}\n",
    "#                     row = {\n",
    "#                         \"reference_term\": reference_term,\n",
    "#                         \"model\": pl.get(\"model\", model_name),\n",
    "#                         \"db\": pl.get(\"db\", db_name),\n",
    "#                         \"field\": pl.get(\"field\", target_field),\n",
    "#                         \"score\": float(h.score),\n",
    "#                         \"cutoff_used\": float(threshold),\n",
    "#                         \"text\": pl.get(\"text\", \"\"),\n",
    "#                     }\n",
    "#                     for k, v in pl.items():\n",
    "#                         if k not in row:\n",
    "#                             row[k] = v\n",
    "#                     rows.append(row)\n",
    "\n",
    "#     df = pd.DataFrame(rows)\n",
    "#     return df.sort_values(\"score\", ascending=False).reset_index(drop=True) if not df.empty else df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0abe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct_values(\n",
    "    client: QdrantClient,\n",
    "    collection_name: str,\n",
    "    field_name: str,\n",
    "    limit: int = 1000,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Get distinct values for a payload field using Qdrant's Facet API.\n",
    "    \n",
    "    Args:\n",
    "        client: QdrantClient instance\n",
    "        collection_name: Name of the collection\n",
    "        field_name: Payload field to get distinct values from\n",
    "        limit: Maximum number of unique values to return\n",
    "    \n",
    "    Returns:\n",
    "        List of distinct values for the field\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use Qdrant Facet API (available in Qdrant 1.12+)\n",
    "        facet_result = client.facet(\n",
    "            collection_name=collection_name,\n",
    "            key=field_name,\n",
    "            limit=limit,\n",
    "        )\n",
    "        \n",
    "        # Extract unique values from facet hits\n",
    "        return [hit.value for hit in facet_result.hits]\n",
    "    \n",
    "    except Exception as e:\n",
    "        LOG.warning(f\"Facet API failed for field '{field_name}': {e!r}, falling back to scroll method\")\n",
    "        \n",
    "        # Fallback: scroll through points and collect unique values\n",
    "        unique_values = set()\n",
    "        offset = None\n",
    "        \n",
    "        while True:\n",
    "            records, offset = client.scroll(\n",
    "                collection_name=collection_name,\n",
    "                limit=100,\n",
    "                offset=offset,\n",
    "                with_payload=[field_name],\n",
    "                with_vectors=False,\n",
    "            )\n",
    "            \n",
    "            if not records:\n",
    "                break\n",
    "                \n",
    "            for record in records:\n",
    "                if record.payload and field_name in record.payload:\n",
    "                    value = record.payload[field_name]\n",
    "                    if isinstance(value, list):\n",
    "                        unique_values.update(value)\n",
    "                    else:\n",
    "                        unique_values.add(value)\n",
    "            \n",
    "            if offset is None:\n",
    "                break\n",
    "        \n",
    "        return list(unique_values)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def search_reference_term_all_models_FAST(\n",
    "    client: QdrantClient,\n",
    "    reference_term: str,\n",
    "    target_field: str,\n",
    "    model_cache: Dict[str, \"SentenceTransformer\"],\n",
    "    limit_per_model: int = 50,\n",
    "    use_knee_cutoff: bool = True,\n",
    "    db_whitelist: Optional[List[str]] = None,\n",
    "    hnsw_ef: int = 64,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fast version with optimized HNSW search parameters.\n",
    "    \"\"\"\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    for model_name, model in model_cache.items():\n",
    "        coll = model_to_collection(model_name)\n",
    "        if not client.collection_exists(coll):\n",
    "            LOG.warning(f\"[{coll}] collection missing; skipping model='{model_name}'\")\n",
    "            continue\n",
    "\n",
    "        q_vec = model.encode(\n",
    "            reference_term,\n",
    "            convert_to_tensor=True,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "\n",
    "        # ? Use facet API to get distinct db values if no whitelist provided\n",
    "        if db_whitelist is not None:\n",
    "            dbs = db_whitelist\n",
    "        else:\n",
    "            dbs = distinct_values(client, coll, \"db\")\n",
    "            LOG.info(f\"[{coll}] Found {len(dbs)} databases: {dbs}\")\n",
    "\n",
    "        for db_name in dbs:\n",
    "            flt = qm.Filter(must=[\n",
    "                qm.FieldCondition(key=\"model\", match=qm.MatchValue(value=model_name)),\n",
    "                qm.FieldCondition(key=\"db\", match=qm.MatchValue(value=db_name)),\n",
    "                qm.FieldCondition(key=\"field\", match=qm.MatchValue(value=target_field)),\n",
    "            ])\n",
    "\n",
    "            hits = client.search(\n",
    "                collection_name=coll,\n",
    "                query_vector=q_vec,\n",
    "                limit=limit_per_model,\n",
    "                with_payload=True,\n",
    "                query_filter=flt,\n",
    "                search_params=qm.SearchParams(\n",
    "                    hnsw_ef=hnsw_ef,\n",
    "                    exact=False,\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            if not hits:\n",
    "                continue\n",
    "\n",
    "            scores = np.array([h.score for h in hits], dtype=np.float32)\n",
    "            if use_knee_cutoff and len(scores) > 1:\n",
    "                sorted_scores = np.sort(scores)[::-1]\n",
    "                try:\n",
    "                    from kneed import KneeLocator\n",
    "                    knee = KneeLocator(range(len(sorted_scores)), sorted_scores,\n",
    "                                       curve=\"convex\", direction=\"decreasing\", S=0.3)\n",
    "                    threshold = float(sorted_scores[knee.knee])\n",
    "                except Exception:\n",
    "                    threshold = 0.0\n",
    "            else:\n",
    "                threshold = 0.0\n",
    "\n",
    "            for h in hits:\n",
    "                if h.score >= threshold:\n",
    "                    pl = h.payload or {}\n",
    "                    row = {\n",
    "                        \"reference_term\": reference_term,\n",
    "                        \"model\": pl.get(\"model\", model_name),\n",
    "                        \"db\": pl.get(\"db\", db_name),\n",
    "                        \"field\": pl.get(\"field\", target_field),\n",
    "                        \"score\": float(h.score),\n",
    "                        \"cutoff_used\": float(threshold),\n",
    "                        \"text\": pl.get(\"text\", \"\"),\n",
    "                    }\n",
    "                    for k, v in pl.items():\n",
    "                        if k not in row:\n",
    "                            row[k] = v\n",
    "                    rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df.sort_values(\"score\", ascending=False).reset_index(drop=True) if not df.empty else df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f5a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Setup\n",
    "cfg = QdrantConfig(index_text_field=False)  # Fast version\n",
    "client = get_client(cfg)\n",
    "model_names = [\n",
    "    # \"FremyCompany/BioLORD-2023-S\",\n",
    "    \"malteos/scincl\",\n",
    "    \"pritamdeka/S-PubMedBERT-MS-MARCO\",\n",
    "    # \"cambridgeltl/SapBERT-from-PubMedBERT-fulltext\",\n",
    "    \"nuvocare/WikiMedical_sent_biobert\",\n",
    "]\n",
    "model_cache = {name: SentenceTransformer(name) for name in model_names}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc2fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(set(df[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8e11b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAST search with optimized HNSW parameters\n",
    "df = search_reference_term_all_models_FAST(\n",
    "    client=client,\n",
    "    reference_term=\"cancer\",\n",
    "    target_field=\"disease_name\",\n",
    "    model_cache=model_cache,\n",
    "    limit_per_model=200,\n",
    "    use_knee_cutoff=True,\n",
    "    db_whitelist=[\"hcdt\", \"ttd\", \"ctd\"],\n",
    "    # db_whitelist=[\"ttd\"],\n",
    "    hnsw_ef=512,  # ? Tune: 32=fastest, 64=balanced, 128=accurate\n",
    ")\n",
    "\n",
    "# print(df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7792bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06f929",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54f04fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8349a795",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(df[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086334d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b960f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(df[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed42d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # =========================\n",
    "# # Usage Example\n",
    "# # =========================\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "# query = \"Tuberculosis treatment\"\n",
    "# model_name = model_names[0]\n",
    "# query_embedding = model_cache[model_name].encode(query)\n",
    "\n",
    "# results = search_fast(\n",
    "#     client=client,\n",
    "#     collection_name=model_to_collection(model_name),\n",
    "#     query_vector=query_embedding,\n",
    "#     limit=200,\n",
    "#     hnsw_ef=64,  # ? Adjust for speed/accuracy tradeoff\n",
    "#     filter_conditions=qm.Filter(\n",
    "#         must=[\n",
    "#             qm.FieldCondition(\n",
    "#                 key=\"db\",\n",
    "#                 match=qm.MatchAny(any=[\"hcdt\", \"ttd\", \"ctd\"])\n",
    "#             )\n",
    "#         ]\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# print(f\"Found {len(results)} results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125d8c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db0c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from qdrant_client import QdrantClient\n",
    "\n",
    "# client = QdrantClient(host=\"localhost\", port=6333)\n",
    "# collection_name = \"emb_malteos_scincl\"\n",
    "# client.recreate_collection(\n",
    "#     collection_name=collection_name,\n",
    "#     vector_size=768,  # use your model's embedding dimension\n",
    "#     distance=\"Cosine\",\n",
    "# )\n",
    "\n",
    "# for record in records:  # your data loader here\n",
    "#     client.upsert(\n",
    "#         collection_name=collection_name,\n",
    "#         points=[{\n",
    "#             \"id\": record[\"id\"],\n",
    "#             \"vector\": record[\"embedding\"],\n",
    "#             \"payload\": {\n",
    "#                 \"text\": record[\"text\"],  # THIS FIELD IS NEEDED!\n",
    "#                 # other metadata\n",
    "#             }\n",
    "#         }]\n",
    "#     )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biochirp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
